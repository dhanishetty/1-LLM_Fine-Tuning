{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, evaluate\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"dhanishetty/albert-xxlarge-v2-Merged\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: 'Negative', 1:'Neutral', 2:'Positive'}\n",
    "label2id = {'Negative':0, 'Neutral':1, 'Positive':2}\n",
    "\n",
    "#generate classification model from model_checkpoints\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels = 3, id2label = id2label, label2id = label2id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"mteb/tweet_sentiment_extraction\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text', 'label', 'label_text'],\n",
       "    num_rows: 3534\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model predictions:\n",
      "----------------------------\n",
      "[1, 2, 0, 2, 2, 2, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 2, 1, 0, 1, 0, 1, 0, 2, 0, 1, 1, 0, 1]\n",
      "[1, 2, 0, 2, 2, 2, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 1, 0, 2, 1, 1, 2, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Trained model predictions:\")\n",
    "print(\"----------------------------\")\n",
    "x =0\n",
    "predictions_list = []\n",
    "labels_list = []\n",
    "while x <=30:\n",
    "    # tokenize text\n",
    "    inputs = tokenizer.encode(dataset['text'][x], return_tensors=\"pt\")\n",
    "    # compute logits\n",
    "    logits = model(inputs).logits\n",
    "    # convert logits to label\n",
    "    predictions = torch.argmax(logits)\n",
    "    predictions_list.append(predictions.item())\n",
    "    labels_list.append(dataset['label'][x])    \n",
    "    x = x+1\n",
    "\n",
    "print(predictions_list)\n",
    "print(labels_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuarcy of the model is 0.8709677419354839\n"
     ]
    }
   ],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "results = accuracy_metric.compute(references=labels_list, predictions=predictions_list)\n",
    "print(f\"Accuarcy of the model is {results[\"accuracy\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87\n",
      "0.87\n",
      "0.87\n",
      "{'f1': array([0.89655172, 0.8       , 0.92307692])}\n"
     ]
    }
   ],
   "source": [
    "f1_metric = evaluate.load(\"f1\")\n",
    "#A multiclass example, with different values for the `average` input.\n",
    "f1_macro = f1_metric.compute(predictions=predictions_list, references=labels_list, average=\"macro\")\n",
    "print(round(f1_macro['f1'], 2))\n",
    "\n",
    "f1_micro = f1_metric.compute(predictions=predictions_list, references=labels_list, average=\"micro\")\n",
    "print(round(f1_micro['f1'], 2))\n",
    "\n",
    "f1_weighted = f1_metric.compute(predictions=predictions_list, references=labels_list, average=\"weighted\")\n",
    "print(round(f1_weighted['f1'], 2))\n",
    "\n",
    "results = f1_metric.compute(predictions=predictions_list, references=labels_list, average=None)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_macro score is 0.888888888888889\n",
      "precision_micro score is 0.8709677419354839\n",
      "precision_weighted score is 0.875268817204301\n",
      "precision_weighted score is {'precision': array([0.86666667, 0.8       , 1.        ])}\n"
     ]
    }
   ],
   "source": [
    "precision_metric = evaluate.load(\"precision\")\n",
    "precision_macro = precision_metric.compute(predictions=predictions_list, references=labels_list, average='macro')\n",
    "print(f\"precision_macro score is {precision_macro[\"precision\"]}\")\n",
    "\n",
    "precision_micro = precision_metric.compute(predictions=predictions_list, references=labels_list, average='micro')\n",
    "print(f\"precision_micro score is {precision_micro[\"precision\"]}\")\n",
    "\n",
    "precision_weighted = precision_metric.compute(predictions=predictions_list, references=labels_list, average='weighted')\n",
    "print(f\"precision_weighted score is {precision_weighted[\"precision\"]}\")\n",
    "\n",
    "precision_none = precision_metric.compute(predictions=predictions_list, references=labels_list, average=None)\n",
    "print(f\"precision_weighted score is {precision_none}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall_macro score is 0.861904761904762\n",
      "recall_micro score is 0.8709677419354839\n",
      "recall_weighted score is 0.8709677419354839\n",
      "recall_None score is {'recall': array([0.92857143, 0.8       , 0.85714286])}\n"
     ]
    }
   ],
   "source": [
    "recall_metric = evaluate.load('recall')\n",
    "recall_macro = recall_metric.compute(predictions=predictions_list, references=labels_list, average='macro')\n",
    "print(f\"recall_macro score is {recall_macro[\"recall\"]}\")\n",
    "\n",
    "recall_micro = recall_metric.compute(predictions=predictions_list, references=labels_list, average='micro')\n",
    "print(f\"recall_micro score is {recall_micro[\"recall\"]}\")\n",
    "\n",
    "recall_weighted = recall_metric.compute(predictions=predictions_list, references=labels_list, average='weighted')\n",
    "print(f\"recall_weighted score is {recall_weighted[\"recall\"]}\")\n",
    "\n",
    "recall_none = recall_metric.compute(predictions=predictions_list, references=labels_list, average=None)\n",
    "print(f\"recall_None score is {recall_none}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
