{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForTokenClassification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnut = load_dataset(\"YurtsAI/named_entity_recognition\", split=\"eval[:10]\")\n",
    "model_id = \"dhanishetty/Google_bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = wnut.features[f\"ner_tags\"].feature.names\n",
    "print(label_list)\n",
    "print(len(label_list))\n",
    "\n",
    "id2label = {}\n",
    "label2id = {}\n",
    "for count, label in enumerate(label_list):\n",
    "    id2label.update({count:label})\n",
    "    label2id.update({label:count})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_id, num_labels=125, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['document_id', 'sentence_id', 'tokens', 'ner_tags'],\n",
      "    num_rows: 10\n",
      "})\n",
      "['*', '*', 'MEMORANDUM', '*', '*', '*', '*', 'TO', ':', '*', '*', 'Personal', 'Journal', '*', '*', 'FROM', ':', '*', '*', '[', 'Your', 'Name', ']', '*', '*', 'DATE', ':', '*', '*', '1961-05-08', '*', '*', 'SUBJECT', ':', '*', '*', 'Daily', 'Reflections', 'and', 'Observations', '1', '.', '*', '*', 'Re', ':', 'Project', 'Alpha', '*', '*', ':', 'Met', 'w/', 'team', '@', '0900', 'hrs', '.', 'Discussed', 'Q3', 'deliverables', '.', 'Action', 'items', ':', 'finalize', 'SOW', 'by', 'EOD', '10/12', '.', '2', '.', '*', '*', 'Re', ':', 'Budget', 'Review', '*', '*', ':', 'FY24', 'budget', 'mtg', '.', 'w/', 'CFO', '@', '1400', 'hrs', '.', 'Key', 'points', ':', 'reduce', 'OPEX', 'by', '5', '%', ',', 'reallocate', 'CAPEX', 'to', 'R', '&', 'D', '.', '3', '.', '*', '*', 'Re', ':', 'Client', 'Meeting', '*', '*', ':', 'Conf', '.', 'call', 'w/', 'XYZ', 'Corp.', '@', '1100', 'hrs', '.', 'Discussed', 'contract', 'renewal', '.', 'Follow-up', 'req', \"'d\", 'by', 'COB', '10/15', '.', '4', '.', '*', '*', 'Re', ':', 'Team', 'Performance', '*', '*', ':', 'Weekly', 'performance', 'review', '.', 'Noted', ':', 'increase', 'in', 'KPIs', 'by', '7', '%', '.', 'Areas', 'for', 'improvement', ':', 'TAT', 'on', 'support', 'tickets', '.', '5', '.', '*', '*', 'Re', ':', 'Training', 'Session', '*', '*', ':', 'Attended', 'cybersecurity', 'training', '@', '1300', 'hrs', '.', 'Key', 'takeaway', ':', 'implement', 'MFA', 'by', '11/01', '.', '6', '.', '*', '*', 'Re', ':', 'Personal', 'Development', '*', '*', ':', 'Completed', 'online', 'course', 'on', 'data', 'analytics', '.', 'Next', 'steps', ':', 'apply', 'learnings', 'to', 'ongoing', 'projects', '.', '7', '.', '*', '*', 'Re', ':', 'Health', '&', 'Wellness', '*', '*', ':', 'Gym', 'session', '@', '0600', 'hrs', '.', 'Focus', ':', 'cardio', 'and', 'strength', 'training', '.', 'Goal', ':', 'improve', 'stamina', '.', '8', '.', '*', '*', 'Re', ':', 'Upcoming', 'Events', '*', '*', ':', 'Plan', 'for', 'Q4', 'strategy', 'session', 'on', '10/20', '.', 'Prep', 'materials', 'and', 'agenda', 'by', '10/18', '.', '9', '.', '*', '*', 'Re', ':', 'Miscellaneous', '*', '*', ':', 'Received', 'feedback', 'on', 'recent', 'presentation', '.', 'Positive', 'remarks', 'on', 'clarity', 'and', 'conciseness', '.', '*', '*', 'END', 'OF', 'MEMORANDUM', '*', '*']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 83, 84, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 24, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 83, 84, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 83, 84, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 24, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 83, 84, 0, 0, 0, 0, 0, 0, 0, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 83, 84, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 0, 0, 0, 0, 0, 0, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(wnut)\n",
    "#print(wnut['document_id'][1])\n",
    "#print(wnut['sentence_id'][1])\n",
    "print(wnut['tokens'][1])\n",
    "print(wnut['ner_tags'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document_id', 'sentence_id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_wnut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "0\n",
      "['*', '*', 'MEMORANDUM', '*', '*', '*', '*', 'TO', ':', '*', '*', 'Personal', 'Journal', '*', '*', 'FROM', ':', '*', '*', '[', 'Your', 'Name', ']', '*', '*', 'DATE', ':', '*', '*', '1961-05-08', '*', '*', 'SUBJECT', ':', '*', '*', 'Daily', 'Reflections', 'and', 'Observations', '1', '.', '*', '*', 'Re', ':', 'Project', 'Alpha', '*', '*', ':', 'Met', 'w/', 'team', '@', '0900', 'hrs', '.', 'Discussed', 'Q3', 'deliverables', '.', 'Action', 'items', ':', 'finalize', 'SOW', 'by', 'EOD', '10/12', '.', '2', '.', '*', '*', 'Re', ':', 'Budget', 'Review', '*', '*', ':', 'FY24', 'budget', 'mtg', '.', 'w/', 'CFO', '@', '1400', 'hrs', '.', 'Key', 'points', ':', 'reduce', 'OPEX', 'by', '5', '%', ',', 'reallocate', 'CAPEX', 'to', 'R', '&', 'D', '.', '3', '.', '*', '*', 'Re', ':', 'Client', 'Meeting', '*', '*', ':', 'Conf', '.', 'call', 'w/', 'XYZ', 'Corp.', '@', '1100', 'hrs', '.', 'Discussed', 'contract', 'renewal', '.', 'Follow-up', 'req', \"'d\", 'by', 'COB', '10/15', '.', '4', '.', '*', '*', 'Re', ':', 'Team', 'Performance', '*', '*', ':', 'Weekly', 'performance', 'review', '.', 'Noted', ':', 'increase', 'in', 'KPIs', 'by', '7', '%', '.', 'Areas', 'for', 'improvement', ':', 'TAT', 'on', 'support', 'tickets', '.', '5', '.', '*', '*', 'Re', ':', 'Training', 'Session', '*', '*', ':', 'Attended', 'cybersecurity', 'training', '@', '1300', 'hrs', '.', 'Key', 'takeaway', ':', 'implement', 'MFA', 'by', '11/01', '.', '6', '.', '*', '*', 'Re', ':', 'Personal', 'Development', '*', '*', ':', 'Completed', 'online', 'course', 'on', 'data', 'analytics', '.', 'Next', 'steps', ':', 'apply', 'learnings', 'to', 'ongoing', 'projects', '.', '7', '.', '*', '*', 'Re', ':', 'Health', '&', 'Wellness', '*', '*', ':', 'Gym', 'session', '@', '0600', 'hrs', '.', 'Focus', ':', 'cardio', 'and', 'strength', 'training', '.', 'Goal', ':', 'improve', 'stamina', '.', '8', '.', '*', '*', 'Re', ':', 'Upcoming', 'Events', '*', '*', ':', 'Plan', 'for', 'Q4', 'strategy', 'session', 'on', '10/20', '.', 'Prep', 'materials', 'and', 'agenda', 'by', '10/18', '.', '9', '.', '*', '*', 'Re', ':', 'Miscellaneous', '*', '*', ':', 'Received', 'feedback', 'on', 'recent', 'presentation', '.', 'Positive', 'remarks', 'on', 'clarity', 'and', 'conciseness', '.', '*', '*', 'END', 'OF', 'MEMORANDUM', '*', '*']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 83, 84, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 24, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 83, 84, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 83, 84, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 24, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 83, 84, 0, 0, 0, 0, 0, 0, 0, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 83, 84, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 0, 0, 0, 0, 0, 0, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[101, 1008, 1008, 20336, 1008, 1008, 1008, 1008, 2000, 1024, 1008, 1008, 3167, 3485, 1008, 1008, 2013, 1024, 1008, 1008, 1031, 2115, 2171, 1033, 1008, 1008, 3058, 1024, 1008, 1008, 3777, 1011, 5709, 1011, 5511, 1008, 1008, 3395, 1024, 1008, 1008, 3679, 16055, 1998, 9420, 1015, 1012, 1008, 1008, 2128, 1024, 2622, 6541, 1008, 1008, 1024, 2777, 1059, 1013, 2136, 1030, 5641, 8889, 17850, 2015, 1012, 6936, 1053, 2509, 8116, 3085, 2015, 1012, 2895, 5167, 1024, 2345, 4697, 2061, 2860, 2011, 1041, 7716, 2184, 1013, 2260, 1012, 1016, 1012, 1008, 1008, 2128, 1024, 5166, 3319, 1008, 1008, 1024, 1042, 2100, 18827, 5166, 11047, 2290, 1012, 1059, 1013, 12935, 2080, 1030, 20652, 17850, 2015, 1012, 3145, 2685, 1024, 5547, 6728, 10288, 2011, 1019, 1003, 1010, 2613, 4135, 16280, 4880, 2595, 2000, 1054, 1004, 1040, 1012, 1017, 1012, 1008, 1008, 2128, 1024, 7396, 3116, 1008, 1008, 1024, 9530, 2546, 1012, 2655, 1059, 1013, 1060, 2100, 2480, 13058, 1012, 1030, 22096, 17850, 2015, 1012, 6936, 3206, 14524, 1012, 3582, 1011, 2039, 2128, 4160, 1005, 1040, 2011, 2522, 2497, 2184, 1013, 2321, 1012, 1018, 1012, 1008, 1008, 2128, 1024, 2136, 2836, 1008, 1008, 1024, 4882, 2836, 3319, 1012, 3264, 1024, 3623, 1999, 1047, 18136, 2011, 1021, 1003, 1012, 2752, 2005, 7620, 1024, 11937, 2102, 2006, 2490, 9735, 1012, 1019, 1012, 1008, 1008, 2128, 1024, 2731, 5219, 1008, 1008, 1024, 3230, 16941, 3366, 10841, 15780, 2731, 1030, 19527, 17850, 2015, 1012, 3145, 2202, 9497, 1024, 10408, 26913, 2011, 2340, 1013, 5890, 1012, 1020, 1012, 1008, 1008, 2128, 1024, 3167, 2458, 1008, 1008, 1024, 2949, 3784, 2607, 2006, 2951, 25095, 1012, 2279, 4084, 1024, 6611, 4083, 2015, 2000, 7552, 3934, 1012, 1021, 1012, 1008, 1008, 2128, 1024, 2740, 1004, 25860, 1008, 1008, 1024, 9726, 5219, 1030, 5757, 8889, 17850, 2015, 1012, 3579, 1024, 4003, 3695, 1998, 3997, 2731, 1012, 3125, 1024, 5335, 2358, 27651, 1012, 1022, 1012, 1008, 1008, 2128, 1024, 9046, 2824, 1008, 1008, 1024, 2933, 2005, 1053, 2549, 5656, 5219, 2006, 2184, 1013, 2322, 1012, 17463, 4475, 1998, 11376, 2011, 2184, 1013, 2324, 1012, 1023, 1012, 1008, 1008, 2128, 1024, 25408, 1008, 1008, 1024, 2363, 12247, 2006, 3522, 8312, 1012, 3893, 12629, 2006, 15563, 1998, 9530, 18380, 2791, 1012, 1008, 1008, 2203, 1997, 20336, 1008, 1008, 102]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, -100, -100, -100, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 83, -100, 84, -100, 0, 0, 0, -100, 0, -100, -100, 0, 0, 0, 0, 0, -100, 0, -100, 0, 23, -100, 24, -100, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, -100, 0, 0, -100, 0, 0, -100, 0, -100, 0, 83, 84, -100, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 0, 0, -100, -100, 0, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, -100, 0, -100, -100, 0, -100, 0, 83, 84, -100, 0, 0, 0, 0, 0, 0, -100, -100, 0, -100, 0, -100, 0, 23, -100, 24, -100, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, -100, -100, 0, 0, 83, 84, -100, 0, 0, 0, -100, 0, 0, 0, 0, 23, -100, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 83, -100, 84, -100, 0, 0, 0, 0, -100, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 23, -100, -100, 0, 0, 0, 0, 0, 0, 23, -100, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, -100, 0, 0, 0, 0, 0, 0, 0, 0, -100]\n",
      "373\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_wnut['document_id'][1])\n",
    "print(tokenized_wnut['sentence_id'][1])\n",
    "print(tokenized_wnut['tokens'][1])\n",
    "print(tokenized_wnut['ner_tags'][1])\n",
    "print(tokenized_wnut['input_ids'][1])\n",
    "print(tokenized_wnut['token_type_ids'][1])\n",
    "print(tokenized_wnut['attention_mask'][1])\n",
    "print(tokenized_wnut['labels'][1])\n",
    "print(len(tokenized_wnut['labels'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_wnut['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1008,  1008, 20336,  1008,  1008,  1008,  1008,  2000,  1024,\n",
      "          1008,  1008,  3167,  3485,  1008,  1008,  2013,  1024,  1008,  1008,\n",
      "          1031,  2115,  2171,  1033,  1008,  1008,  3058,  1024,  1008,  1008,\n",
      "          3777,  1011,  5709,  1011,  5511,  1008,  1008,  3395,  1024,  1008,\n",
      "          1008,  3679, 16055,  1998,  9420,  1015,  1012,  1008,  1008,  2128,\n",
      "          1024,  2622,  6541,  1008,  1008,  1024,  2777,  1059,  1013,  2136,\n",
      "          1030,  5641,  8889, 17850,  2015,  1012,  6936,  1053,  2509,  8116,\n",
      "          3085,  2015,  1012,  2895,  5167,  1024,  2345,  4697,  2061,  2860,\n",
      "          2011,  1041,  7716,  2184,  1013,  2260,  1012,  1016,  1012,  1008,\n",
      "          1008,  2128,  1024,  5166,  3319,  1008,  1008,  1024,  1042,  2100,\n",
      "         18827,  5166, 11047,  2290,  1012,  1059,  1013, 12935,  2080,  1030,\n",
      "         20652, 17850,  2015,  1012,  3145,  2685,  1024,  5547,  6728, 10288,\n",
      "          2011,  1019,  1003,  1010,  2613,  4135, 16280,  4880,  2595,  2000,\n",
      "          1054,  1004,  1040,  1012,  1017,  1012,  1008,  1008,  2128,  1024,\n",
      "          7396,  3116,  1008,  1008,  1024,  9530,  2546,  1012,  2655,  1059,\n",
      "          1013,  1060,  2100,  2480, 13058,  1012,  1030, 22096, 17850,  2015,\n",
      "          1012,  6936,  3206, 14524,  1012,  3582,  1011,  2039,  2128,  4160,\n",
      "          1005,  1040,  2011,  2522,  2497,  2184,  1013,  2321,  1012,  1018,\n",
      "          1012,  1008,  1008,  2128,  1024,  2136,  2836,  1008,  1008,  1024,\n",
      "          4882,  2836,  3319,  1012,  3264,  1024,  3623,  1999,  1047, 18136,\n",
      "          2011,  1021,  1003,  1012,  2752,  2005,  7620,  1024, 11937,  2102,\n",
      "          2006,  2490,  9735,  1012,  1019,  1012,  1008,  1008,  2128,  1024,\n",
      "          2731,  5219,  1008,  1008,  1024,  3230, 16941,  3366, 10841, 15780,\n",
      "          2731,  1030, 19527, 17850,  2015,  1012,  3145,  2202,  9497,  1024,\n",
      "         10408, 26913,  2011,  2340,  1013,  5890,  1012,  1020,  1012,  1008,\n",
      "          1008,  2128,  1024,  3167,  2458,  1008,  1008,  1024,  2949,  3784,\n",
      "          2607,  2006,  2951, 25095,  1012,  2279,  4084,  1024,  6611,  4083,\n",
      "          2015,  2000,  7552,  3934,  1012,  1021,  1012,  1008,  1008,  2128,\n",
      "          1024,  2740,  1004, 25860,  1008,  1008,  1024,  9726,  5219,  1030,\n",
      "          5757,  8889, 17850,  2015,  1012,  3579,  1024,  4003,  3695,  1998,\n",
      "          3997,  2731,  1012,  3125,  1024,  5335,  2358, 27651,  1012,  1022,\n",
      "          1012,  1008,  1008,  2128,  1024,  9046,  2824,  1008,  1008,  1024,\n",
      "          2933,  2005,  1053,  2549,  5656,  5219,  2006,  2184,  1013,  2322,\n",
      "          1012, 17463,  4475,  1998, 11376,  2011,  2184,  1013,  2324,  1012,\n",
      "          1023,  1012,  1008,  1008,  2128,  1024, 25408,  1008,  1008,  1024,\n",
      "          2363, 12247,  2006,  3522,  8312,  1012,  3893, 12629,  2006, 15563,\n",
      "          1998,  9530, 18380,  2791,  1012,  1008,  1008,  2203,  1997, 20336,\n",
      "          1008,  1008,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "373\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dhanishetty/Google_bert-base-uncased\")\n",
    "inputs = tokenizer(wnut['tokens'][1], return_tensors=\"pt\", is_split_into_words=True)\n",
    "print(inputs)\n",
    "print((inputs['input_ids'].size(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits are tensor([[[10.6675,  0.1120, -0.2097,  ..., -1.4871, -1.2241, -0.5975],\n",
      "         [11.9228,  0.0135, -0.5378,  ..., -1.3693, -0.7376, -0.6716],\n",
      "         [11.8895,  0.1200, -0.6164,  ..., -1.3939, -0.7246, -0.6989],\n",
      "         ...,\n",
      "         [11.9703, -0.2312, -0.5486,  ..., -1.4095, -0.9093, -0.6575],\n",
      "         [11.9416, -0.0595, -0.5946,  ..., -1.4339, -0.7784, -0.7288],\n",
      "         [11.1518,  0.3179, -0.1241,  ..., -1.7010, -1.1350, -0.7336]]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dhanishetty/Google_bert-base-uncased\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "    print(f\"logits are {logits}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 16, 16, 16, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 83, 84, 84, 84, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 24, 24, 24, 24, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 83, 84, 84, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 83, 84, 84, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 24, 24, 24, 24, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 83, 84, 84, 0, 0, 0, 0, 0, 0, 0, 0, 23, 24, 24, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 83, 84, 84, 84, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 0, 24, 0, 0, 0, 0, 0, 0, 23, 24, 24, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, -100, -100, -100, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 83, -100, 84, -100, 0, 0, 0, -100, 0, -100, -100, 0, 0, 0, 0, 0, -100, 0, -100, 0, 23, -100, 24, -100, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, -100, 0, 0, -100, 0, 0, -100, 0, -100, 0, 83, 84, -100, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 0, 0, -100, -100, 0, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, -100, 0, -100, -100, 0, -100, 0, 83, 84, -100, 0, 0, 0, 0, 0, 0, -100, -100, 0, -100, 0, -100, 0, 23, -100, 24, -100, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, -100, -100, 0, 0, 83, 84, -100, 0, 0, 0, -100, 0, 0, 0, 0, 23, -100, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 83, -100, 84, -100, 0, 0, 0, 0, -100, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, 23, -100, -100, 0, 0, 0, 0, 0, 0, 23, -100, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, -100, 0, 0, 0, 0, 0, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "predictions = torch.argmax(logits, dim=2)\n",
    "predictions = predictions.tolist()\n",
    "\n",
    "predictions = predictions[0]\n",
    "print((predictions))\n",
    "print((tokenized_wnut['labels'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_list = predictions\n",
    "labels_list = tokenized_wnut['labels'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuarcy of the model is 0.8364611260053619\n"
     ]
    }
   ],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "results = accuracy_metric.compute(references=labels_list, predictions=predictions_list)\n",
    "print(f\"Accuarcy of the model is {results[\"accuracy\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n",
      "0.84\n",
      "0.78\n",
      "{'f1': array([0.        , 0.93779904, 1.        , 0.        , 1.        ,\n",
      "       0.26666667, 1.        , 0.58823529])}\n"
     ]
    }
   ],
   "source": [
    "f1_metric = evaluate.load(\"f1\")\n",
    "#A multiclass example, with different values for the `average` input.\n",
    "f1_macro = f1_metric.compute(predictions=predictions_list, references=labels_list, average=\"macro\")\n",
    "print(round(f1_macro['f1'], 2))\n",
    "\n",
    "f1_micro = f1_metric.compute(predictions=predictions_list, references=labels_list, average=\"micro\")\n",
    "print(round(f1_micro['f1'], 2))\n",
    "\n",
    "f1_weighted = f1_metric.compute(predictions=predictions_list, references=labels_list, average=\"weighted\")\n",
    "print(round(f1_weighted['f1'], 2))\n",
    "\n",
    "results = f1_metric.compute(predictions=predictions_list, references=labels_list, average=None)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_macro score is 0.5566744629244629\n",
      "precision_micro score is 0.8364611260053619\n",
      "precision_weighted score is 0.7317924750900622\n",
      "precision_weighted score is {'precision': array([0.        , 0.88288288, 1.        , 0.        , 1.        ,\n",
      "       0.15384615, 1.        , 0.41666667])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\dhani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\dhani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "precision_metric = evaluate.load(\"precision\")\n",
    "precision_macro = precision_metric.compute(predictions=predictions_list, references=labels_list, average='macro')\n",
    "print(f\"precision_macro score is {precision_macro[\"precision\"]}\")\n",
    "\n",
    "precision_micro = precision_metric.compute(predictions=predictions_list, references=labels_list, average='micro')\n",
    "print(f\"precision_micro score is {precision_micro[\"precision\"]}\")\n",
    "\n",
    "precision_weighted = precision_metric.compute(predictions=predictions_list, references=labels_list, average='weighted')\n",
    "print(f\"precision_weighted score is {precision_weighted[\"precision\"]}\")\n",
    "\n",
    "precision_none = precision_metric.compute(predictions=predictions_list, references=labels_list, average=None)\n",
    "print(f\"precision_weighted score is {precision_none}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall_macro score is 0.75\n",
      "recall_micro score is 0.8364611260053619\n",
      "recall_weighted score is 0.8364611260053619\n",
      "recall_None score is {'recall': array([0., 1., 1., 0., 1., 1., 1., 1.])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\dhani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\dhani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "recall_metric = evaluate.load('recall')\n",
    "recall_macro = recall_metric.compute(predictions=predictions_list, references=labels_list, average='macro')\n",
    "print(f\"recall_macro score is {recall_macro[\"recall\"]}\")\n",
    "\n",
    "recall_micro = recall_metric.compute(predictions=predictions_list, references=labels_list, average='micro')\n",
    "print(f\"recall_micro score is {recall_micro[\"recall\"]}\")\n",
    "\n",
    "recall_weighted = recall_metric.compute(predictions=predictions_list, references=labels_list, average='weighted')\n",
    "print(f\"recall_weighted score is {recall_weighted[\"recall\"]}\")\n",
    "\n",
    "recall_none = recall_metric.compute(predictions=predictions_list, references=labels_list, average=None)\n",
    "print(f\"recall_None score is {recall_none}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
