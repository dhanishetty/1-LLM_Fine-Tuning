{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wnut = load_dataset(\"YurtsAI/named_entity_recognition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document_id', 'sentence_id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 39999\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['document_id', 'sentence_id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['document_id', 'sentence_id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 4999\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['Subject', ':', 'üéâ', 'Welcome', 'to', 'Our', 'Exclusive', 'Loyalty', 'Program', '!', 'üéâ', 'Hey', 'there', ',', 'We', '‚Äô', 're', 'super', 'excited', 'to', 'announce', 'our', 'brand', 'new', 'customer', 'loyalty', 'program', 'at', 'Bella', \"'s\", 'Bistro', '!', 'üçù', 'Starting', 'from', 'October', '15th', ',', '2023', ',', 'you', 'can', 'start', 'earning', 'points', 'every', 'time', 'you', 'dine', 'with', 'us', '.', 'It', \"'s\", 'our', 'way', 'of', 'saying', 'thanks', 'for', 'being', 'such', 'an', 'awesome', 'customer', '!', 'Here', '‚Äô', 's', 'the', 'lowdown', ':', 'For', 'every', '$', '1', 'you', 'spend', ',', 'you', '‚Äô', 'll', 'earn', '10', 'points', '.', 'Rack', 'up', '500', 'points', 'and', 'you', '‚Äô', 'll', 'get', 'a', '$', '25', 'gift', 'card', 'to', 'use', 'on', 'your', 'next', 'visit', '.', 'Plus', ',', 'we', '‚Äô', 've', 'got', 'some', 'sweet', 'perks', 'like', 'birthday', 'freebies', ',', 'early', 'access', 'to', 'special', 'events', ',', 'and', 'members-only', 'discounts', '.', 'üéÅ', 'To', 'join', ',', 'just', 'sign', 'up', 'at', 'the', 'front', 'desk', 'next', 'time', 'you', '‚Äô', 're', 'in', ',', 'or', 'hit', 'up', 'our', 'website', 'and', 'click', 'on', 'the', '‚Äú', 'Loyalty', 'Program', '‚Äù', 'tab', '.', 'It', '‚Äô', 's', 'that', 'easy', '!', 'And', 'don', '‚Äô', 't', 'worry', ',', 'we', '‚Äô', 'll', 'keep', 'you', 'posted', 'on', 'your', 'points', 'balance', 'and', 'special', 'offers', 'via', 'email', '.', 'Got', 'any', 'questions', '?', 'Shoot', 'us', 'an', 'email', 'at', 'loyalty', '@', 'bellasbistro.com', 'or', 'give', 'us', 'a', 'ring', 'at', '(', '555', ')', '123-4567', '.', 'We', 'can', '‚Äô', 't', 'wait', 'to', 'see', 'you', 'soon', 'and', 'start', 'rewarding', 'you', 'for', 'being', 'part', 'of', 'the', 'Bella', '‚Äô', 's', 'family', '!', 'Cheers', ',', 'The', 'Bella', '‚Äô', 's', 'Bistro', 'Team', 'üç∑', 'P.S', '.', 'Don', '‚Äô', 't', 'forget', 'to', 'follow', 'us', 'on', 'Instagram', '@', 'bellasbistro', 'for', 'the', 'latest', 'updates', 'and', 'mouth-watering', 'pics', '!', 'üì∏']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 33, 34, 34, 0, 0, 0, 0, 83, 84, 84, 84, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(wnut[\"train\"]['document_id'][0])\n",
    "print(wnut[\"train\"]['tokens'][0])\n",
    "print(wnut[\"train\"]['ner_tags'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-GENERAL__product-software', 'I-GENERAL__product-software', 'B-GENERAL__person-scholar', 'I-GENERAL__person-scholar', 'B-GENERAL__event-attack/battle/war/militaryconflict', 'I-GENERAL__event-attack/battle/war/militaryconflict', 'B-DATETIME__range', 'I-DATETIME__range', 'B-GENERAL__other-award', 'I-GENERAL__other-award', 'B-GENERAL__other-disease', 'I-GENERAL__other-disease', 'B-GENERAL__building-sportsfacility', 'I-GENERAL__building-sportsfacility', 'B-DATETIME__authored', 'I-DATETIME__authored', 'B-GENERAL__location-bodiesofwater', 'I-GENERAL__location-bodiesofwater', 'B-GENERAL__art-writtenart', 'I-GENERAL__art-writtenart', 'B-GENERAL__other-god', 'I-GENERAL__other-god', 'B-DATETIME__relative', 'I-DATETIME__relative', 'B-GENERAL__organization-government/governmentagency', 'I-GENERAL__organization-government/governmentagency', 'B-GENERAL__product-weapon', 'I-GENERAL__product-weapon', 'B-GENERAL__other-language', 'I-GENERAL__other-language', 'B-GENERAL__event-other', 'I-GENERAL__event-other', 'B-GENERAL__building-restaurant', 'I-GENERAL__building-restaurant', 'B-GENERAL__location-mountain', 'I-GENERAL__location-mountain', 'B-GENERAL__organization-religion', 'I-GENERAL__organization-religion', 'B-GENERAL__location-park', 'I-GENERAL__location-park', 'B-GENERAL__event-sportsevent', 'I-GENERAL__event-sportsevent', 'B-GENERAL__other-livingthing', 'I-GENERAL__other-livingthing', 'B-GENERAL__product-food', 'I-GENERAL__product-food', 'B-GENERAL__other-currency', 'I-GENERAL__other-currency', 'B-GENERAL__location-island', 'I-GENERAL__location-island', 'B-GENERAL__organization-education', 'I-GENERAL__organization-education', 'B-GENERAL__other-astronomything', 'I-GENERAL__other-astronomything', 'B-GENERAL__other-chemicalthing', 'I-GENERAL__other-chemicalthing', 'B-GENERAL__other-law', 'I-GENERAL__other-law', 'B-GENERAL__building-library', 'I-GENERAL__building-library', 'B-GENERAL__organization-media/newspaper', 'I-GENERAL__organization-media/newspaper', 'B-GENERAL__location-GPE', 'I-GENERAL__location-GPE', 'B-GENERAL__art-music', 'I-GENERAL__art-music', 'B-GENERAL__art-broadcastprogram', 'I-GENERAL__art-broadcastprogram', 'B-GENERAL__person-soldier', 'I-GENERAL__person-soldier', 'B-GENERAL__building-airport', 'I-GENERAL__building-airport', 'B-GENERAL__other-biologything', 'I-GENERAL__other-biologything', 'B-GENERAL__art-other', 'I-GENERAL__art-other', 'B-GENERAL__person-other', 'I-GENERAL__person-other', 'B-GENERAL__organization-other', 'I-GENERAL__organization-other', 'B-GENERAL__product-game', 'I-GENERAL__product-game', 'B-DATETIME__absolute', 'I-DATETIME__absolute', 'B-GENERAL__organization-sportsteam', 'I-GENERAL__organization-sportsteam', 'B-GENERAL__product-train', 'I-GENERAL__product-train', 'B-GENERAL__product-car', 'I-GENERAL__product-car', 'B-GENERAL__other-medical', 'I-GENERAL__other-medical', 'B-GENERAL__building-theater', 'I-GENERAL__building-theater', 'B-GENERAL__art-film', 'I-GENERAL__art-film', 'B-GENERAL__art-painting', 'I-GENERAL__art-painting', 'B-GENERAL__location-other', 'I-GENERAL__location-other', 'B-GENERAL__building-hospital', 'I-GENERAL__building-hospital', 'B-GENERAL__product-airplane', 'I-GENERAL__product-airplane', 'B-GENERAL__product-ship', 'I-GENERAL__product-ship', 'B-GENERAL__person-politician', 'I-GENERAL__person-politician', 'B-GENERAL__other-educationaldegree', 'I-GENERAL__other-educationaldegree', 'B-GENERAL__product-other', 'I-GENERAL__product-other', 'B-GENERAL__building-other', 'I-GENERAL__building-other', 'B-GENERAL__person-artist/author', 'I-GENERAL__person-artist/author', 'B-GENERAL__organization-showorganization', 'I-GENERAL__organization-showorganization', 'B-GENERAL__location-road/railway/highway/transit', 'I-GENERAL__location-road/railway/highway/transit', 'B-GENERAL__building-hotel', 'I-GENERAL__building-hotel', 'B-GENERAL__organization-company', 'I-GENERAL__organization-company']\n",
      "125\n",
      "{0: 'O', 1: 'B-GENERAL__product-software', 2: 'I-GENERAL__product-software', 3: 'B-GENERAL__person-scholar', 4: 'I-GENERAL__person-scholar', 5: 'B-GENERAL__event-attack/battle/war/militaryconflict', 6: 'I-GENERAL__event-attack/battle/war/militaryconflict', 7: 'B-DATETIME__range', 8: 'I-DATETIME__range', 9: 'B-GENERAL__other-award', 10: 'I-GENERAL__other-award', 11: 'B-GENERAL__other-disease', 12: 'I-GENERAL__other-disease', 13: 'B-GENERAL__building-sportsfacility', 14: 'I-GENERAL__building-sportsfacility', 15: 'B-DATETIME__authored', 16: 'I-DATETIME__authored', 17: 'B-GENERAL__location-bodiesofwater', 18: 'I-GENERAL__location-bodiesofwater', 19: 'B-GENERAL__art-writtenart', 20: 'I-GENERAL__art-writtenart', 21: 'B-GENERAL__other-god', 22: 'I-GENERAL__other-god', 23: 'B-DATETIME__relative', 24: 'I-DATETIME__relative', 25: 'B-GENERAL__organization-government/governmentagency', 26: 'I-GENERAL__organization-government/governmentagency', 27: 'B-GENERAL__product-weapon', 28: 'I-GENERAL__product-weapon', 29: 'B-GENERAL__other-language', 30: 'I-GENERAL__other-language', 31: 'B-GENERAL__event-other', 32: 'I-GENERAL__event-other', 33: 'B-GENERAL__building-restaurant', 34: 'I-GENERAL__building-restaurant', 35: 'B-GENERAL__location-mountain', 36: 'I-GENERAL__location-mountain', 37: 'B-GENERAL__organization-religion', 38: 'I-GENERAL__organization-religion', 39: 'B-GENERAL__location-park', 40: 'I-GENERAL__location-park', 41: 'B-GENERAL__event-sportsevent', 42: 'I-GENERAL__event-sportsevent', 43: 'B-GENERAL__other-livingthing', 44: 'I-GENERAL__other-livingthing', 45: 'B-GENERAL__product-food', 46: 'I-GENERAL__product-food', 47: 'B-GENERAL__other-currency', 48: 'I-GENERAL__other-currency', 49: 'B-GENERAL__location-island', 50: 'I-GENERAL__location-island', 51: 'B-GENERAL__organization-education', 52: 'I-GENERAL__organization-education', 53: 'B-GENERAL__other-astronomything', 54: 'I-GENERAL__other-astronomything', 55: 'B-GENERAL__other-chemicalthing', 56: 'I-GENERAL__other-chemicalthing', 57: 'B-GENERAL__other-law', 58: 'I-GENERAL__other-law', 59: 'B-GENERAL__building-library', 60: 'I-GENERAL__building-library', 61: 'B-GENERAL__organization-media/newspaper', 62: 'I-GENERAL__organization-media/newspaper', 63: 'B-GENERAL__location-GPE', 64: 'I-GENERAL__location-GPE', 65: 'B-GENERAL__art-music', 66: 'I-GENERAL__art-music', 67: 'B-GENERAL__art-broadcastprogram', 68: 'I-GENERAL__art-broadcastprogram', 69: 'B-GENERAL__person-soldier', 70: 'I-GENERAL__person-soldier', 71: 'B-GENERAL__building-airport', 72: 'I-GENERAL__building-airport', 73: 'B-GENERAL__other-biologything', 74: 'I-GENERAL__other-biologything', 75: 'B-GENERAL__art-other', 76: 'I-GENERAL__art-other', 77: 'B-GENERAL__person-other', 78: 'I-GENERAL__person-other', 79: 'B-GENERAL__organization-other', 80: 'I-GENERAL__organization-other', 81: 'B-GENERAL__product-game', 82: 'I-GENERAL__product-game', 83: 'B-DATETIME__absolute', 84: 'I-DATETIME__absolute', 85: 'B-GENERAL__organization-sportsteam', 86: 'I-GENERAL__organization-sportsteam', 87: 'B-GENERAL__product-train', 88: 'I-GENERAL__product-train', 89: 'B-GENERAL__product-car', 90: 'I-GENERAL__product-car', 91: 'B-GENERAL__other-medical', 92: 'I-GENERAL__other-medical', 93: 'B-GENERAL__building-theater', 94: 'I-GENERAL__building-theater', 95: 'B-GENERAL__art-film', 96: 'I-GENERAL__art-film', 97: 'B-GENERAL__art-painting', 98: 'I-GENERAL__art-painting', 99: 'B-GENERAL__location-other', 100: 'I-GENERAL__location-other', 101: 'B-GENERAL__building-hospital', 102: 'I-GENERAL__building-hospital', 103: 'B-GENERAL__product-airplane', 104: 'I-GENERAL__product-airplane', 105: 'B-GENERAL__product-ship', 106: 'I-GENERAL__product-ship', 107: 'B-GENERAL__person-politician', 108: 'I-GENERAL__person-politician', 109: 'B-GENERAL__other-educationaldegree', 110: 'I-GENERAL__other-educationaldegree', 111: 'B-GENERAL__product-other', 112: 'I-GENERAL__product-other', 113: 'B-GENERAL__building-other', 114: 'I-GENERAL__building-other', 115: 'B-GENERAL__person-artist/author', 116: 'I-GENERAL__person-artist/author', 117: 'B-GENERAL__organization-showorganization', 118: 'I-GENERAL__organization-showorganization', 119: 'B-GENERAL__location-road/railway/highway/transit', 120: 'I-GENERAL__location-road/railway/highway/transit', 121: 'B-GENERAL__building-hotel', 122: 'I-GENERAL__building-hotel', 123: 'B-GENERAL__organization-company', 124: 'I-GENERAL__organization-company'}\n",
      "{'O': 0, 'B-GENERAL__product-software': 1, 'I-GENERAL__product-software': 2, 'B-GENERAL__person-scholar': 3, 'I-GENERAL__person-scholar': 4, 'B-GENERAL__event-attack/battle/war/militaryconflict': 5, 'I-GENERAL__event-attack/battle/war/militaryconflict': 6, 'B-DATETIME__range': 7, 'I-DATETIME__range': 8, 'B-GENERAL__other-award': 9, 'I-GENERAL__other-award': 10, 'B-GENERAL__other-disease': 11, 'I-GENERAL__other-disease': 12, 'B-GENERAL__building-sportsfacility': 13, 'I-GENERAL__building-sportsfacility': 14, 'B-DATETIME__authored': 15, 'I-DATETIME__authored': 16, 'B-GENERAL__location-bodiesofwater': 17, 'I-GENERAL__location-bodiesofwater': 18, 'B-GENERAL__art-writtenart': 19, 'I-GENERAL__art-writtenart': 20, 'B-GENERAL__other-god': 21, 'I-GENERAL__other-god': 22, 'B-DATETIME__relative': 23, 'I-DATETIME__relative': 24, 'B-GENERAL__organization-government/governmentagency': 25, 'I-GENERAL__organization-government/governmentagency': 26, 'B-GENERAL__product-weapon': 27, 'I-GENERAL__product-weapon': 28, 'B-GENERAL__other-language': 29, 'I-GENERAL__other-language': 30, 'B-GENERAL__event-other': 31, 'I-GENERAL__event-other': 32, 'B-GENERAL__building-restaurant': 33, 'I-GENERAL__building-restaurant': 34, 'B-GENERAL__location-mountain': 35, 'I-GENERAL__location-mountain': 36, 'B-GENERAL__organization-religion': 37, 'I-GENERAL__organization-religion': 38, 'B-GENERAL__location-park': 39, 'I-GENERAL__location-park': 40, 'B-GENERAL__event-sportsevent': 41, 'I-GENERAL__event-sportsevent': 42, 'B-GENERAL__other-livingthing': 43, 'I-GENERAL__other-livingthing': 44, 'B-GENERAL__product-food': 45, 'I-GENERAL__product-food': 46, 'B-GENERAL__other-currency': 47, 'I-GENERAL__other-currency': 48, 'B-GENERAL__location-island': 49, 'I-GENERAL__location-island': 50, 'B-GENERAL__organization-education': 51, 'I-GENERAL__organization-education': 52, 'B-GENERAL__other-astronomything': 53, 'I-GENERAL__other-astronomything': 54, 'B-GENERAL__other-chemicalthing': 55, 'I-GENERAL__other-chemicalthing': 56, 'B-GENERAL__other-law': 57, 'I-GENERAL__other-law': 58, 'B-GENERAL__building-library': 59, 'I-GENERAL__building-library': 60, 'B-GENERAL__organization-media/newspaper': 61, 'I-GENERAL__organization-media/newspaper': 62, 'B-GENERAL__location-GPE': 63, 'I-GENERAL__location-GPE': 64, 'B-GENERAL__art-music': 65, 'I-GENERAL__art-music': 66, 'B-GENERAL__art-broadcastprogram': 67, 'I-GENERAL__art-broadcastprogram': 68, 'B-GENERAL__person-soldier': 69, 'I-GENERAL__person-soldier': 70, 'B-GENERAL__building-airport': 71, 'I-GENERAL__building-airport': 72, 'B-GENERAL__other-biologything': 73, 'I-GENERAL__other-biologything': 74, 'B-GENERAL__art-other': 75, 'I-GENERAL__art-other': 76, 'B-GENERAL__person-other': 77, 'I-GENERAL__person-other': 78, 'B-GENERAL__organization-other': 79, 'I-GENERAL__organization-other': 80, 'B-GENERAL__product-game': 81, 'I-GENERAL__product-game': 82, 'B-DATETIME__absolute': 83, 'I-DATETIME__absolute': 84, 'B-GENERAL__organization-sportsteam': 85, 'I-GENERAL__organization-sportsteam': 86, 'B-GENERAL__product-train': 87, 'I-GENERAL__product-train': 88, 'B-GENERAL__product-car': 89, 'I-GENERAL__product-car': 90, 'B-GENERAL__other-medical': 91, 'I-GENERAL__other-medical': 92, 'B-GENERAL__building-theater': 93, 'I-GENERAL__building-theater': 94, 'B-GENERAL__art-film': 95, 'I-GENERAL__art-film': 96, 'B-GENERAL__art-painting': 97, 'I-GENERAL__art-painting': 98, 'B-GENERAL__location-other': 99, 'I-GENERAL__location-other': 100, 'B-GENERAL__building-hospital': 101, 'I-GENERAL__building-hospital': 102, 'B-GENERAL__product-airplane': 103, 'I-GENERAL__product-airplane': 104, 'B-GENERAL__product-ship': 105, 'I-GENERAL__product-ship': 106, 'B-GENERAL__person-politician': 107, 'I-GENERAL__person-politician': 108, 'B-GENERAL__other-educationaldegree': 109, 'I-GENERAL__other-educationaldegree': 110, 'B-GENERAL__product-other': 111, 'I-GENERAL__product-other': 112, 'B-GENERAL__building-other': 113, 'I-GENERAL__building-other': 114, 'B-GENERAL__person-artist/author': 115, 'I-GENERAL__person-artist/author': 116, 'B-GENERAL__organization-showorganization': 117, 'I-GENERAL__organization-showorganization': 118, 'B-GENERAL__location-road/railway/highway/transit': 119, 'I-GENERAL__location-road/railway/highway/transit': 120, 'B-GENERAL__building-hotel': 121, 'I-GENERAL__building-hotel': 122, 'B-GENERAL__organization-company': 123, 'I-GENERAL__organization-company': 124}\n"
     ]
    }
   ],
   "source": [
    "label_list = wnut[\"train\"].features[f\"ner_tags\"].feature.names\n",
    "print(label_list)\n",
    "print(len(label_list))\n",
    "\n",
    "id2label = {}\n",
    "label2id = {}\n",
    "for count, label in enumerate(label_list):\n",
    "    id2label.update({count:label})\n",
    "    label2id.update({label:count})\n",
    "print(id2label)\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The letter that prefixes each `ner_tag` indicates the token position of the entity:\n",
    "\n",
    "- `B-` indicates the beginning of an entity.\n",
    "- `I-` indicates a token is contained inside the same entity (for example, the `State` token is a part of an entity like\n",
    "  `Empire State Building`).\n",
    "- `0` indicates the token doesn't correspond to any entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'subject', ':', '[UNK]', 'welcome', 'to', 'our', 'exclusive', 'loyalty', 'program', '!', '[UNK]', 'hey', 'there', ',', 'we', '‚Äô', 're', 'super', 'excited', 'to', 'announce', 'our', 'brand', 'new', 'customer', 'loyalty', 'program', 'at', 'bella', \"'\", 's', 'bis', '##tro', '!', '[UNK]', 'starting', 'from', 'october', '15th', ',', '202', '##3', ',', 'you', 'can', 'start', 'earning', 'points', 'every', 'time', 'you', 'din', '##e', 'with', 'us', '.', 'it', \"'\", 's', 'our', 'way', 'of', 'saying', 'thanks', 'for', 'being', 'such', 'an', 'awesome', 'customer', '!', 'here', '‚Äô', 's', 'the', 'low', '##down', ':', 'for', 'every', '$', '1', 'you', 'spend', ',', 'you', '‚Äô', 'll', 'earn', '10', 'points', '.', 'rack', 'up', '500', 'points', 'and', 'you', '‚Äô', 'll', 'get', 'a', '$', '25', 'gift', 'card', 'to', 'use', 'on', 'your', 'next', 'visit', '.', 'plus', ',', 'we', '‚Äô', 've', 'got', 'some', 'sweet', 'per', '##ks', 'like', 'birthday', 'free', '##bies', ',', 'early', 'access', 'to', 'special', 'events', ',', 'and', 'members', '-', 'only', 'discount', '##s', '.', '[UNK]', 'to', 'join', ',', 'just', 'sign', 'up', 'at', 'the', 'front', 'desk', 'next', 'time', 'you', '‚Äô', 're', 'in', ',', 'or', 'hit', 'up', 'our', 'website', 'and', 'click', 'on', 'the', '‚Äú', 'loyalty', 'program', '‚Äù', 'tab', '.', 'it', '‚Äô', 's', 'that', 'easy', '!', 'and', 'don', '‚Äô', 't', 'worry', ',', 'we', '‚Äô', 'll', 'keep', 'you', 'posted', 'on', 'your', 'points', 'balance', 'and', 'special', 'offers', 'via', 'email', '.', 'got', 'any', 'questions', '?', 'shoot', 'us', 'an', 'email', 'at', 'loyalty', '@', 'bella', '##sb', '##ist', '##ro', '.', 'com', 'or', 'give', 'us', 'a', 'ring', 'at', '(', '555', ')', '123', '-', '45', '##6', '##7', '.', 'we', 'can', '‚Äô', 't', 'wait', 'to', 'see', 'you', 'soon', 'and', 'start', 'reward', '##ing', 'you', 'for', 'being', 'part', 'of', 'the', 'bella', '‚Äô', 's', 'family', '!', 'cheers', ',', 'the', 'bella', '‚Äô', 's', 'bis', '##tro', 'team', '[UNK]', 'p', '.', 's', '.', 'don', '‚Äô', 't', 'forget', 'to', 'follow', 'us', 'on', 'ins', '##tagram', '@', 'bella', '##sb', '##ist', '##ro', 'for', 'the', 'latest', 'updates', 'and', 'mouth', '-', 'watering', 'pic', '##s', '!', '[UNK]', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "example = wnut[\"train\"][0]\n",
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"google-bert/bert-base-uncased\", num_labels=125, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "labels = [label_list[i] for i in example[f\"ner_tags\"]]\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_wnut_model\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "        do_eval=True,\n",
    "        optim=\"adamw_torch\",\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=4,\n",
    "        per_device_eval_batch_size=8,\n",
    "        log_level=\"debug\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=10,\n",
    "        learning_rate=5e-5,\n",
    "        #fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        #bf16 = torch.cuda.is_bf16_supported(),\n",
    "        eval_steps=30,\n",
    "        num_train_epochs=1,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type=\"inverse_sqrt\",\n",
    "        report_to=\"wandb\",\n",
    "        seed=42,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_wnut[\"train\"],\n",
    "    eval_dataset=tokenized_wnut[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    #compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Currently training with a batch size of: 8\n",
      "The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 39,999\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 1,250\n",
      "  Number of trainable parameters = 108,987,773\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdhanishetty\u001b[0m (\u001b[33mdhanishetty-personaluse\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\dhani\\OneDrive\\Desktop\\AI - Projects\\PEFT Fine Tuning\\NER\\wandb\\run-20240728_173229-5uvs4op2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dhanishetty-personaluse/huggingface/runs/5uvs4op2' target=\"_blank\">my_awesome_wnut_model</a></strong> to <a href='https://wandb.ai/dhanishetty-personaluse/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dhanishetty-personaluse/huggingface' target=\"_blank\">https://wandb.ai/dhanishetty-personaluse/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dhanishetty-personaluse/huggingface/runs/5uvs4op2' target=\"_blank\">https://wandb.ai/dhanishetty-personaluse/huggingface/runs/5uvs4op2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9efacfaffd5c4d8c9c059b13437d50ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.9177, 'grad_norm': 9.90449047088623, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.01}\n",
      "{'loss': 4.1744, 'grad_norm': 12.41919231414795, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2234, 'grad_norm': 10.41997241973877, 'learning_rate': 1.2e-05, 'epoch': 0.02}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2643808d2e431fbe82bec4303ffd16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9557464718818665, 'eval_runtime': 88.1607, 'eval_samples_per_second': 56.715, 'eval_steps_per_second': 7.089, 'epoch': 0.02}\n",
      "{'loss': 0.675, 'grad_norm': 1.0343400239944458, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.03}\n",
      "{'loss': 0.4893, 'grad_norm': 0.4469161331653595, 'learning_rate': 2e-05, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4601, 'grad_norm': 0.34064096212387085, 'learning_rate': 2.4e-05, 'epoch': 0.05}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc4e65103a74c2190583e7f408418f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4359284043312073, 'eval_runtime': 90.6478, 'eval_samples_per_second': 55.159, 'eval_steps_per_second': 6.895, 'epoch': 0.05}\n",
      "{'loss': 0.4374, 'grad_norm': 0.5447105169296265, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.06}\n",
      "{'loss': 0.3573, 'grad_norm': 0.5032377243041992, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3149, 'grad_norm': 0.470638632774353, 'learning_rate': 3.6e-05, 'epoch': 0.07}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6340d2e41a0a4586b5bb2088aa338e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.27947095036506653, 'eval_runtime': 92.8036, 'eval_samples_per_second': 53.877, 'eval_steps_per_second': 6.735, 'epoch': 0.07}\n",
      "{'loss': 0.3011, 'grad_norm': 0.5721376538276672, 'learning_rate': 4e-05, 'epoch': 0.08}\n",
      "{'loss': 0.2709, 'grad_norm': 0.38807451725006104, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2403, 'grad_norm': 0.7627832889556885, 'learning_rate': 4.8e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d28025bcba24b3bb8423a2c8b4516d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21418051421642303, 'eval_runtime': 93.4005, 'eval_samples_per_second': 53.533, 'eval_steps_per_second': 6.692, 'epoch': 0.1}\n",
      "{'loss': 0.2181, 'grad_norm': 0.39021122455596924, 'learning_rate': 4.902903378454601e-05, 'epoch': 0.1}\n",
      "{'loss': 0.2066, 'grad_norm': 0.6714336276054382, 'learning_rate': 4.72455591261534e-05, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1876, 'grad_norm': 0.5277619361877441, 'learning_rate': 4.564354645876385e-05, 'epoch': 0.12}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac46f9e68f4f44ce959ab39cf3991088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1714152842760086, 'eval_runtime': 93.9757, 'eval_samples_per_second': 53.205, 'eval_steps_per_second': 6.651, 'epoch': 0.12}\n",
      "{'loss': 0.1822, 'grad_norm': 0.3209989070892334, 'learning_rate': 4.4194173824159226e-05, 'epoch': 0.13}\n",
      "{'loss': 0.1587, 'grad_norm': 0.36568185687065125, 'learning_rate': 4.2874646285627205e-05, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1691, 'grad_norm': 0.25880080461502075, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.14}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f663ed98312d4fd6b5855d729ed4f58d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.14884047210216522, 'eval_runtime': 94.5431, 'eval_samples_per_second': 52.886, 'eval_steps_per_second': 6.611, 'epoch': 0.14}\n",
      "{'loss': 0.1645, 'grad_norm': 0.443061500787735, 'learning_rate': 4.055535528269064e-05, 'epoch': 0.15}\n",
      "{'loss': 0.1439, 'grad_norm': 0.42140403389930725, 'learning_rate': 3.952847075210474e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1394, 'grad_norm': 0.4118771255016327, 'learning_rate': 3.857583749052298e-05, 'epoch': 0.17}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d2806873b664d60a12b7edacfef404a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.12734360992908478, 'eval_runtime': 95.0555, 'eval_samples_per_second': 52.601, 'eval_steps_per_second': 6.575, 'epoch': 0.17}\n",
      "{'loss': 0.1291, 'grad_norm': 0.3343525230884552, 'learning_rate': 3.7688918072220455e-05, 'epoch': 0.18}\n",
      "{'loss': 0.1169, 'grad_norm': 0.2932218015193939, 'learning_rate': 3.686048903872428e-05, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1264, 'grad_norm': 0.28592023253440857, 'learning_rate': 3.608439182435161e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e20864b6b6f44c2a5b6c5e1890e58f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11595567315816879, 'eval_runtime': 95.153, 'eval_samples_per_second': 52.547, 'eval_steps_per_second': 6.568, 'epoch': 0.19}\n",
      "{'loss': 0.1277, 'grad_norm': 0.4229891300201416, 'learning_rate': 3.535533905932738e-05, 'epoch': 0.2}\n",
      "{'loss': 0.1156, 'grad_norm': 0.27077800035476685, 'learning_rate': 3.466876226407682e-05, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1113, 'grad_norm': 0.23715676367282867, 'learning_rate': 3.402069087198859e-05, 'epoch': 0.22}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c63c5628c747b4a383b40989f313eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.10786109417676926, 'eval_runtime': 95.2943, 'eval_samples_per_second': 52.469, 'eval_steps_per_second': 6.559, 'epoch': 0.22}\n",
      "{'loss': 0.1123, 'grad_norm': 0.3651641607284546, 'learning_rate': 3.3407655239053046e-05, 'epoch': 0.22}\n",
      "{'loss': 0.1081, 'grad_norm': 0.29335781931877136, 'learning_rate': 3.282660821493064e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1148, 'grad_norm': 0.32116663455963135, 'learning_rate': 3.2274861218395145e-05, 'epoch': 0.24}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f93c7ed5c2437d895eb0fed641d295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.09916553646326065, 'eval_runtime': 95.2902, 'eval_samples_per_second': 52.471, 'eval_steps_per_second': 6.559, 'epoch': 0.24}\n",
      "{'loss': 0.1, 'grad_norm': 0.43275970220565796, 'learning_rate': 3.1750031750047625e-05, 'epoch': 0.25}\n",
      "{'loss': 0.0996, 'grad_norm': 0.5081122517585754, 'learning_rate': 3.125e-05, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0995, 'grad_norm': 0.3384336531162262, 'learning_rate': 3.0772872744833186e-05, 'epoch': 0.26}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46b80a5cb0264fdda9090d71ee7ae012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.09402385354042053, 'eval_runtime': 95.5319, 'eval_samples_per_second': 52.339, 'eval_steps_per_second': 6.542, 'epoch': 0.26}\n",
      "{'loss': 0.1101, 'grad_norm': 0.384884238243103, 'learning_rate': 3.031695312954162e-05, 'epoch': 0.27}\n",
      "{'loss': 0.0956, 'grad_norm': 0.2910580337047577, 'learning_rate': 2.988071523335984e-05, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.096, 'grad_norm': 0.3370329439640045, 'learning_rate': 2.9462782549439484e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc9b7dd692a49bd8688acd9c8696c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.09407379478216171, 'eval_runtime': 95.7259, 'eval_samples_per_second': 52.232, 'eval_steps_per_second': 6.529, 'epoch': 0.29}\n",
      "{'loss': 0.0993, 'grad_norm': 0.39330679178237915, 'learning_rate': 2.9061909685954824e-05, 'epoch': 0.3}\n",
      "{'loss': 0.0941, 'grad_norm': 0.24604696035385132, 'learning_rate': 2.8676966733820225e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0954, 'grad_norm': 0.41135162115097046, 'learning_rate': 2.8306925853614895e-05, 'epoch': 0.31}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "340c27096a7845579a7455c314fcf30a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08541449904441833, 'eval_runtime': 95.7161, 'eval_samples_per_second': 52.238, 'eval_steps_per_second': 6.53, 'epoch': 0.31}\n",
      "{'loss': 0.0869, 'grad_norm': 0.3805256485939026, 'learning_rate': 2.7950849718747376e-05, 'epoch': 0.32}\n",
      "{'loss': 0.098, 'grad_norm': 0.4700911343097687, 'learning_rate': 2.7607881518711635e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.089, 'grad_norm': 0.31023702025413513, 'learning_rate': 2.7277236279499045e-05, 'epoch': 0.34}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d0787aaf794294b002bbefa7bb87b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08985783159732819, 'eval_runtime': 95.7688, 'eval_samples_per_second': 52.209, 'eval_steps_per_second': 6.526, 'epoch': 0.34}\n",
      "{'loss': 0.0901, 'grad_norm': 0.30388543009757996, 'learning_rate': 2.6958193300859607e-05, 'epoch': 0.34}\n",
      "{'loss': 0.0838, 'grad_norm': 0.4077324867248535, 'learning_rate': 2.6650089544451305e-05, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0826, 'grad_norm': 0.31819066405296326, 'learning_rate': 2.6352313834736496e-05, 'epoch': 0.36}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8198b9a44bf5481eb16d9234f8b11c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08412227779626846, 'eval_runtime': 98.1464, 'eval_samples_per_second': 50.944, 'eval_steps_per_second': 6.368, 'epoch': 0.36}\n",
      "{'loss': 0.0839, 'grad_norm': 0.31286558508872986, 'learning_rate': 2.6064301757134345e-05, 'epoch': 0.37}\n",
      "{'loss': 0.0911, 'grad_norm': 0.3003038167953491, 'learning_rate': 2.578553115646984e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0872, 'grad_norm': 0.32445192337036133, 'learning_rate': 2.5515518153991443e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10c651cfa11842a09575b76f72aa43c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08109450340270996, 'eval_runtime': 95.5679, 'eval_samples_per_second': 52.319, 'eval_steps_per_second': 6.54, 'epoch': 0.38}\n",
      "{'loss': 0.0951, 'grad_norm': 0.3692586421966553, 'learning_rate': 2.5253813613805265e-05, 'epoch': 0.39}\n",
      "{'loss': 0.0833, 'grad_norm': 0.39296185970306396, 'learning_rate': 2.5e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0794, 'grad_norm': 0.2961695194244385, 'learning_rate': 2.4753688574416857e-05, 'epoch': 0.41}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4023e63987ba4643bc6c2a2b20c82a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07591819018125534, 'eval_runtime': 95.4623, 'eval_samples_per_second': 52.377, 'eval_steps_per_second': 6.547, 'epoch': 0.41}\n",
      "{'loss': 0.0813, 'grad_norm': 0.41920268535614014, 'learning_rate': 2.4514516892273005e-05, 'epoch': 0.42}\n",
      "{'loss': 0.0767, 'grad_norm': 0.32657432556152344, 'learning_rate': 2.4282146558931605e-05, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0756, 'grad_norm': 0.28314897418022156, 'learning_rate': 2.4056261216234405e-05, 'epoch': 0.43}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ec075c5d5b400b937777db5e46bded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07660890370607376, 'eval_runtime': 95.591, 'eval_samples_per_second': 52.306, 'eval_steps_per_second': 6.538, 'epoch': 0.43}\n",
      "{'loss': 0.0797, 'grad_norm': 0.30566734075546265, 'learning_rate': 2.383656473113981e-05, 'epoch': 0.44}\n",
      "{'loss': 0.0815, 'grad_norm': 0.44152235984802246, 'learning_rate': 2.36227795630767e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0826, 'grad_norm': 0.2717912793159485, 'learning_rate': 2.341464528954235e-05, 'epoch': 0.46}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d830e826424ff7abd7d145b568417d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07285229861736298, 'eval_runtime': 95.7297, 'eval_samples_per_second': 52.23, 'eval_steps_per_second': 6.529, 'epoch': 0.46}\n",
      "{'loss': 0.0841, 'grad_norm': 0.3223874270915985, 'learning_rate': 2.3211917272131486e-05, 'epoch': 0.46}\n",
      "{'loss': 0.0824, 'grad_norm': 0.33856770396232605, 'learning_rate': 2.301436544745809e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0841, 'grad_norm': 0.3864303529262543, 'learning_rate': 2.2821773229381925e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f101a8cac347bea16507cf4201afbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07150722295045853, 'eval_runtime': 95.7312, 'eval_samples_per_second': 52.23, 'eval_steps_per_second': 6.529, 'epoch': 0.48}\n",
      "{'loss': 0.0724, 'grad_norm': 0.2857248783111572, 'learning_rate': 2.2633936510629633e-05, 'epoch': 0.49}\n",
      "{'loss': 0.0713, 'grad_norm': 0.37248024344444275, 'learning_rate': 2.2450662753346862e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.076, 'grad_norm': 0.2778356075286865, 'learning_rate': 2.22717701593687e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f4f37190f564e1393dc40f5b62bfc00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0736687108874321, 'eval_runtime': 95.7616, 'eval_samples_per_second': 52.213, 'eval_steps_per_second': 6.527, 'epoch': 0.5}\n",
      "{'loss': 0.0696, 'grad_norm': 0.25716230273246765, 'learning_rate': 2.2097086912079613e-05, 'epoch': 0.51}\n",
      "{'loss': 0.0762, 'grad_norm': 0.35245102643966675, 'learning_rate': 2.1926450482675732e-05, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0746, 'grad_norm': 0.34155288338661194, 'learning_rate': 2.175970699446223e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e602e8920da24d958afec830824f1c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06906171888113022, 'eval_runtime': 95.7604, 'eval_samples_per_second': 52.214, 'eval_steps_per_second': 6.527, 'epoch': 0.53}\n",
      "{'loss': 0.0749, 'grad_norm': 0.2651234269142151, 'learning_rate': 2.1596710639534002e-05, 'epoch': 0.54}\n",
      "{'loss': 0.0776, 'grad_norm': 0.3137856721878052, 'learning_rate': 2.1437323142813602e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0719, 'grad_norm': 0.40775513648986816, 'learning_rate': 2.1281413268968714e-05, 'epoch': 0.55}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d44452a1a14dbe8b45e89f23e5cd95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06967412680387497, 'eval_runtime': 96.1269, 'eval_samples_per_second': 52.015, 'eval_steps_per_second': 6.502, 'epoch': 0.55}\n",
      "{'loss': 0.0726, 'grad_norm': 0.3233773112297058, 'learning_rate': 2.1128856368212917e-05, 'epoch': 0.56}\n",
      "{'loss': 0.0705, 'grad_norm': 0.285941481590271, 'learning_rate': 2.097953395741723e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0722, 'grad_norm': 0.29153862595558167, 'learning_rate': 2.0833333333333336e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27df2658357942b1bc9bd6e43c5863c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06733627617359161, 'eval_runtime': 95.8533, 'eval_samples_per_second': 52.163, 'eval_steps_per_second': 6.52, 'epoch': 0.58}\n",
      "{'loss': 0.0752, 'grad_norm': 0.3015619218349457, 'learning_rate': 2.06901472150592e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0708, 'grad_norm': 0.3185776174068451, 'learning_rate': 2.0549873413169665e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0713, 'grad_norm': 0.33445772528648376, 'learning_rate': 2.0412414523193156e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6231a3d780b4417d88df465713f36ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06563226878643036, 'eval_runtime': 96.1632, 'eval_samples_per_second': 51.995, 'eval_steps_per_second': 6.499, 'epoch': 0.6}\n",
      "{'loss': 0.0635, 'grad_norm': 0.2805996239185333, 'learning_rate': 2.027767764134532e-05, 'epoch': 0.61}\n",
      "{'loss': 0.0676, 'grad_norm': 0.25092166662216187, 'learning_rate': 2.0145574100634505e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0671, 'grad_norm': 0.41168585419654846, 'learning_rate': 2.0016019225635894e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c593e1418b5543dc8a2c400b3be29766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06524062156677246, 'eval_runtime': 95.9363, 'eval_samples_per_second': 52.118, 'eval_steps_per_second': 6.515, 'epoch': 0.62}\n",
      "{'loss': 0.0717, 'grad_norm': 0.3585234582424164, 'learning_rate': 1.9888932104393253e-05, 'epoch': 0.63}\n",
      "{'loss': 0.0742, 'grad_norm': 0.3386789560317993, 'learning_rate': 1.976423537605237e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0741, 'grad_norm': 0.30379876494407654, 'learning_rate': 1.9641855032959654e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ab8dd924fa1412c98eca915cacefc9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06750655174255371, 'eval_runtime': 96.2378, 'eval_samples_per_second': 51.955, 'eval_steps_per_second': 6.494, 'epoch': 0.65}\n",
      "{'loss': 0.0803, 'grad_norm': 0.38160353899002075, 'learning_rate': 1.9521720236075758e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0726, 'grad_norm': 0.46411585807800293, 'learning_rate': 1.940376314265832e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0723, 'grad_norm': 0.26500216126441956, 'learning_rate': 1.928791874526149e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d4db1f5b9704b31b7a8dfa38cf3d717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06631357222795486, 'eval_runtime': 96.0793, 'eval_samples_per_second': 52.04, 'eval_steps_per_second': 6.505, 'epoch': 0.67}\n",
      "{'loss': 0.0666, 'grad_norm': 0.28501251339912415, 'learning_rate': 1.917412472118426e-05, 'epoch': 0.68}\n",
      "{'loss': 0.0565, 'grad_norm': 0.2570032477378845, 'learning_rate': 1.9062321291575584e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0687, 'grad_norm': 0.3425997495651245, 'learning_rate': 1.8952451089472586e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d031361a8c14ff2afa9325be75bf212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06494321674108505, 'eval_runtime': 96.1946, 'eval_samples_per_second': 51.978, 'eval_steps_per_second': 6.497, 'epoch': 0.7}\n",
      "{'loss': 0.0676, 'grad_norm': 0.3043898940086365, 'learning_rate': 1.8844459036110228e-05, 'epoch': 0.7}\n",
      "{'loss': 0.0682, 'grad_norm': 0.33488571643829346, 'learning_rate': 1.873829222489654e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.067, 'grad_norm': 0.3252585530281067, 'learning_rate': 1.8633899812498247e-05, 'epoch': 0.72}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fab54f754dd4dff8035d182acda57ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06366347521543503, 'eval_runtime': 96.264, 'eval_samples_per_second': 51.94, 'eval_steps_per_second': 6.493, 'epoch': 0.72}\n",
      "{'loss': 0.0641, 'grad_norm': 0.284531831741333, 'learning_rate': 1.853123291652753e-05, 'epoch': 0.73}\n",
      "{'loss': 0.0671, 'grad_norm': 0.2929619252681732, 'learning_rate': 1.843024451936214e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0623, 'grad_norm': 0.31288209557533264, 'learning_rate': 1.833088937766916e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "881794b2ab23406ca41da2add65cf768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06425225734710693, 'eval_runtime': 95.8811, 'eval_samples_per_second': 52.148, 'eval_steps_per_second': 6.518, 'epoch': 0.74}\n",
      "{'loss': 0.0697, 'grad_norm': 0.3102553188800812, 'learning_rate': 1.823312393723682e-05, 'epoch': 0.75}\n",
      "{'loss': 0.0621, 'grad_norm': 0.30535757541656494, 'learning_rate': 1.8136906252750293e-05, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0599, 'grad_norm': 0.28860846161842346, 'learning_rate': 1.8042195912175806e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf6f9396aca4cf6a4b2befb8937b325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06426078826189041, 'eval_runtime': 95.742, 'eval_samples_per_second': 52.224, 'eval_steps_per_second': 6.528, 'epoch': 0.77}\n",
      "{'loss': 0.0676, 'grad_norm': 0.34121981263160706, 'learning_rate': 1.7948953965443456e-05, 'epoch': 0.78}\n",
      "{'loss': 0.0636, 'grad_norm': 0.36336925625801086, 'learning_rate': 1.785714285714286e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0686, 'grad_norm': 0.326299250125885, 'learning_rate': 1.7766726362967538e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6001705e91e743b9959ecbde7ae9788a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06235704943537712, 'eval_runtime': 95.6664, 'eval_samples_per_second': 52.265, 'eval_steps_per_second': 6.533, 'epoch': 0.79}\n",
      "{'loss': 0.0631, 'grad_norm': 0.33014631271362305, 'learning_rate': 1.767766952966369e-05, 'epoch': 0.8}\n",
      "{'loss': 0.0727, 'grad_norm': 0.3273756802082062, 'learning_rate': 1.75899386182573e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0638, 'grad_norm': 0.3181602954864502, 'learning_rate': 1.7503501050350124e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9738de4faec4e88b122c1beed596b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06225712224841118, 'eval_runtime': 95.7445, 'eval_samples_per_second': 52.222, 'eval_steps_per_second': 6.528, 'epoch': 0.82}\n",
      "{'loss': 0.0626, 'grad_norm': 0.4268520474433899, 'learning_rate': 1.741832535729044e-05, 'epoch': 0.82}\n",
      "{'loss': 0.0677, 'grad_norm': 0.2858629822731018, 'learning_rate': 1.733438113203841e-05, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0565, 'grad_norm': 0.31830134987831116, 'learning_rate': 1.7251638983558856e-05, 'epoch': 0.84}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e99b47a943b408a8d1d673b89fe149e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06258691102266312, 'eval_runtime': 95.8511, 'eval_samples_per_second': 52.164, 'eval_steps_per_second': 6.521, 'epoch': 0.84}\n",
      "{'loss': 0.0623, 'grad_norm': 0.37158316373825073, 'learning_rate': 1.717007049358613e-05, 'epoch': 0.85}\n",
      "{'loss': 0.0665, 'grad_norm': 0.2898356020450592, 'learning_rate': 1.708964817561658e-05, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0614, 'grad_norm': 0.3442445397377014, 'learning_rate': 1.7010345435994294e-05, 'epoch': 0.86}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4606869127114288aac35b0fadf6b5d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06150106340646744, 'eval_runtime': 95.8783, 'eval_samples_per_second': 52.149, 'eval_steps_per_second': 6.519, 'epoch': 0.86}\n",
      "{'loss': 0.0623, 'grad_norm': 0.2743555009365082, 'learning_rate': 1.693213653696491e-05, 'epoch': 0.87}\n",
      "{'loss': 0.0628, 'grad_norm': 0.33757779002189636, 'learning_rate': 1.685499656158105e-05, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.063, 'grad_norm': 0.3253263831138611, 'learning_rate': 1.677890138035061e-05, 'epoch': 0.89}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ea1d7a48f14bd5b588b05810b73ecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.059201691299676895, 'eval_runtime': 95.9023, 'eval_samples_per_second': 52.136, 'eval_steps_per_second': 6.517, 'epoch': 0.89}\n",
      "{'loss': 0.0609, 'grad_norm': 0.35458680987358093, 'learning_rate': 1.6703827619526523e-05, 'epoch': 0.9}\n",
      "{'loss': 0.0643, 'grad_norm': 0.3138948678970337, 'learning_rate': 1.6629752630943485e-05, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0592, 'grad_norm': 0.4406217634677887, 'learning_rate': 1.6556654463313047e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491e933180704f5b966d376d016030c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06179441511631012, 'eval_runtime': 96.1634, 'eval_samples_per_second': 51.995, 'eval_steps_per_second': 6.499, 'epoch': 0.91}\n",
      "{'loss': 0.0681, 'grad_norm': 0.2683200538158417, 'learning_rate': 1.648451183489468e-05, 'epoch': 0.92}\n",
      "{'loss': 0.0639, 'grad_norm': 0.31618234515190125, 'learning_rate': 1.641330410746532e-05, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0687, 'grad_norm': 0.3998905420303345, 'learning_rate': 1.6343011261515337e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c17674e1611423f810a1ce32336f768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.061761219054460526, 'eval_runtime': 96.097, 'eval_samples_per_second': 52.031, 'eval_steps_per_second': 6.504, 'epoch': 0.94}\n",
      "{'loss': 0.061, 'grad_norm': 0.25416696071624756, 'learning_rate': 1.6273613872602983e-05, 'epoch': 0.94}\n",
      "{'loss': 0.0566, 'grad_norm': 0.25004205107688904, 'learning_rate': 1.620509308880411e-05, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0577, 'grad_norm': 0.2766192853450775, 'learning_rate': 1.6137430609197572e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b10d6500022422da402750b3330ffe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05996313318610191, 'eval_runtime': 95.8707, 'eval_samples_per_second': 52.154, 'eval_steps_per_second': 6.519, 'epoch': 0.96}\n",
      "{'loss': 0.0643, 'grad_norm': 0.309196799993515, 'learning_rate': 1.6070608663330624e-05, 'epoch': 0.97}\n",
      "{'loss': 0.0632, 'grad_norm': 0.26700371503829956, 'learning_rate': 1.6004609991612e-05, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: sentence_id, ner_tags, tokens, document_id. If sentence_id, ner_tags, tokens, document_id are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0629, 'grad_norm': 0.30616605281829834, 'learning_rate': 1.593941782658346e-05, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aae5982110d4e69a3187105914f82de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.059648316353559494, 'eval_runtime': 95.6266, 'eval_samples_per_second': 52.287, 'eval_steps_per_second': 6.536, 'epoch': 0.98}\n",
      "{'loss': 0.0602, 'grad_norm': 0.3527510464191437, 'learning_rate': 1.5875015875023812e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to my_awesome_wnut_model\\checkpoint-1250\n",
      "Configuration saved in my_awesome_wnut_model\\checkpoint-1250\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0629, 'grad_norm': 0.271147221326828, 'learning_rate': 1.5811388300841898e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in my_awesome_wnut_model\\checkpoint-1250\\model.safetensors\n",
      "tokenizer config file saved in my_awesome_wnut_model\\checkpoint-1250\\tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_wnut_model\\checkpoint-1250\\special_tokens_map.json\n",
      "Saving model checkpoint to my_awesome_wnut_model\\checkpoint-1250\n",
      "Configuration saved in my_awesome_wnut_model\\checkpoint-1250\\config.json\n",
      "Model weights saved in my_awesome_wnut_model\\checkpoint-1250\\model.safetensors\n",
      "tokenizer config file saved in my_awesome_wnut_model\\checkpoint-1250\\tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_wnut_model\\checkpoint-1250\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 6298.7585, 'train_samples_per_second': 6.35, 'train_steps_per_second': 0.198, 'train_loss': 0.19616772747039796, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1250, training_loss=0.19616772747039796, metrics={'train_runtime': 6298.7585, 'train_samples_per_second': 6.35, 'train_steps_per_second': 0.198, 'total_flos': 1.0282625919198e+16, 'train_loss': 0.19616772747039796, 'epoch': 1.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to my_awesome_wnut_model\n",
      "Configuration saved in my_awesome_wnut_model\\config.json\n",
      "Model weights saved in my_awesome_wnut_model\\model.safetensors\n",
      "tokenizer config file saved in my_awesome_wnut_model\\tokenizer_config.json\n",
      "Special tokens file saved in my_awesome_wnut_model\\special_tokens_map.json\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Token Classification', 'type': 'token-classification'}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109f12ee14a54551b9e4c5d407752794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3478dd475304c8fba3893ff9421b5c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4175cd681b14c53a2f368fd01fcb11b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.11k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/dhanishetty/my_awesome_wnut_model/commit/4ef537fc57cebe9fb970f0de1084cb822c170e7b', commit_message='dhanishetty/bert-base-uncased_adapters', commit_description='', oid='4ef537fc57cebe9fb970f0de1084cb822c170e7b', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(\"dhanishetty/bert-base-uncased_adapters\" , token= \"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\"\n",
    "                    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
